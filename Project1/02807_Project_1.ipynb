{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/hanlululu/02807-Computational-tools-for-Data-Science/blob/main/Project1/02807_Project_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PlsPJUw7wCWZ"
   },
   "source": [
    "# 02807: Project 1\n",
    " \n",
    "## Practical information\n",
    " \n",
    "* This project must be completed in groups of 3 students.\n",
    "    * The group must be registered on the course site on DTU Learn: My Course > Groups\n",
    "* This project must be handed in as a jupyter notebook to the course site on DTU Learn. \n",
    "    * Go to the Course Content > Assignments tab to upload your submission. \n",
    "* This project is due on Monday, November 1, 20:00.\n",
    "\n",
    "## Submission rules\n",
    "\n",
    "* Each group has to hand in *one* notebook (`.ipynb`) with their solutions, including a filled out Contribution table (see below).\n",
    "* Your solution must be written in Python.\n",
    "* For each question you should use the cells provided (\"`# your code goes here`\") for your solution\n",
    "    * It is allowed to add code cells within a question block, but consider if it's really necessary.\n",
    "* You should not remove the problem statements, and you should not modify the structure of the notebook.\n",
    "* Your notebook should be runnable and readable from top to bottom.\n",
    "    * Meaning that your code cells work when run in order (from top to bottom).\n",
    "    * Output of any cell depends only on itself and cells above it.\n",
    "* Your notebook should be submitted after having been run from top to bottom.\n",
    "    * This means outputs are interpretable without necessarily running your cells.\n",
    "    * The simplest way to achieve this is using the jupyter menu item Kernel > Restart & Run All just prior to submission. If any cell fails when you do this, your notebook is not ready for submission.\n",
    "* Failure to comply may make it impossible for us to evaluate your submission properly, which will likely negatively impact the points awarded.\n",
    "    \n",
    " \n",
    "## Colaboration policy\n",
    " \n",
    "* It is not allowed to collaborate on the exercises with students outside your group, except for discussing the text of the exercise with teachers and fellow students enrolled on the course in the same semester. \n",
    "* It is not allowed to exchange, hand-over or in any other way communicate solutions or parts of solutions to the exercises. \n",
    "* It is not allowed to use solutions from similar courses, or solutions found elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZP30rwxHDQyG"
   },
   "source": [
    "## Contribution table and grading\n",
    "\n",
    "* The total amount of points in the project is 110.\n",
    "* You have to indicate who has solved each part of each exercise in a **contribution table**. \n",
    "* The following is an example of a contributions table:\n",
    "\n",
    "|        | Exercise 1 | Exercise 2 | Exercise 3 | Exercise 4 |\n",
    "|--------|------------|------------|------------|------------|\n",
    "| **Part 1** | John       |    Mary        |     Ann       |   Mary, Ann         |\n",
    "| **Part 2** |     Mary       |    Mary        |   Ann         |    John, Ann        |\n",
    "| **Part 3** |     John, Mary, Ann       |      John, Ann      |   John         | **n.a.**      |\n",
    "| **Part 4** | **n.a.**       |  Ann          |     John, Mary       | **n.a.**       |\n",
    "| **Part 5** | **n.a.**     | John, Mary, Ann           | John       | **n.a.**       |\n",
    "\n",
    "* A group member can take credit for solving a part of an exercise only if they have contributed **substantially** to the solution. \n",
    "    * Simple contributions, such as correcting a small bug or double-checking the results of functions, are not sufficient for taking credit for a solution.\n",
    "    * Several group members can take credit for the same solution if they all have contributed substantially to it.\n",
    "* Each group member must contribute **at least 50 points**. \n",
    "    * If no name is provided for an exercise's part, **all group members** are considered contributors to it.\n",
    "* Group members should decide amongst themselves how to collaborate on the project to meet the above-mentioned constraints.  \n",
    "* Scores are individual. The score $\\text{score}(m)$ for a group member $m$ ranges from 0 to 10 and is calculated as follows: \n",
    "\n",
    "  * $\\text{individual-score}(m) = \\frac{\\text{total number of points for the parts correctly solved by }m}{\\text{total number of points for the parts contributed by }m}$\n",
    "\n",
    "  * $\\text{group-score} = \\frac{\\text{total number of points correctly solved by any group member}}{\\text{total number of points in the project}}$\n",
    "\n",
    "  * $\\text{score}(m) =  7.5 \\cdot \\text{individual-score}(m) + 2.5 \\cdot \\text{group-score}$\n",
    "\n",
    "* **Example**: in the contribution table above, suppose that all parts are solved correctly except for those of Exercise 4, which are both wrong. Then Ann's score is calculated as follows:\n",
    "\n",
    "  * $\\text{individual-score}(Ann) = \\frac{2.5+12.5+12.5+2.5+15+5}{2.5+12.5+12.5+2.5+15+5 + 10 + 10} = \\frac{50}{70} = 0.714$\n",
    "\n",
    "  * $\\text{group-score} = \\frac{90}{110} = 0.818$\n",
    "\n",
    "  * $\\text{score}(Ann) = 7.5\\cdot 0.714 + 2.5 \\cdot 0.818 = 7.4$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vklet8XdRGVV"
   },
   "source": [
    "# Group declaration table \n",
    "\n",
    "This table must be filled before submission.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-27T15:03:19.791917Z",
     "start_time": "2021-09-27T15:03:19.764408Z"
    },
    "id": "chiXA3CzRSA1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "d = {'Exercise 1' : [\"\", \"\", \"\", \"\", \"\"], \n",
    "     'Exercise 2' : [\"\", \"\", \"\", \"\", \"\"],\n",
    "     'Exercise 3' : [\"\", \"\", \"\", \"\", \"\"], \n",
    "     'Exercise 4' : [\"\", \"\", \"\", \"\", \"\"],\n",
    "     } \n",
    "  \n",
    "ct = pd.DataFrame(d, index =['Part 1','Part 2','Part 3','Part 4','Part 5']) \n",
    "\n",
    "ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d7aa1b0ad6979877450f9cd89e1e37289b51cf6e",
    "id": "CF8tC7Z8CgbW"
   },
   "source": [
    "# Introduction to the Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pO8N4lj29u9w"
   },
   "source": [
    "![link text](https://ph-files.imgix.net/069dd825-cddf-4048-adde-8e81396c2c68?auto=format)\n",
    "\n",
    "\n",
    "You will be working with datasets obtained through the [The Movie Database (TMDb) API](https://developers.themoviedb.org/3/getting-started/introduction). The first dataset is part of the MovieLens Latest Full Dataset, comprising 26 million ratings on 45.000 movies from 27.000 users. Let's look at the features in this dataset.\n",
    "\n",
    "**Features**\n",
    "\n",
    "* **adult**: Indicates if the movie is X-Rated.\n",
    "* **belongs_to_collection**: A stringified dictionary with info on the movie series a particular film belongs to (e.g.: Lord of the Rings).\n",
    "* **budget**: The movie budget in dollars.\n",
    "* **genres**: A stringified list of dictionaries describing all genres associated with the movie.\n",
    "* **homepage**: The movie's official homepage.\n",
    "* **id**: An identifier for the movie.\n",
    "* **imdb_id**: IMDB's identifier for the movie.\n",
    "* **original_language**: The language in which the movie was shot.\n",
    "* **original_title**: The original title of the movie.\n",
    "* **overview**: A brief text about the movie.\n",
    "* **popularity**: A Popularity Score given by TMDb.\n",
    "* **poster_path**: The URL of the poster image.\n",
    "* **production_companies**: A stringified list of production companies involved with making of the movie.\n",
    "* **production_countries**: A stringified list of countries in which the movie was produced.\n",
    "* **release_date**: Release date of the movie in theaters.\n",
    "* **revenue**: The total revenue of the movie in dollars.\n",
    "* **runtime**: The runtime of the movie in minutes.\n",
    "* **spoken_languages**: A stringified list of languages spoken in the film.\n",
    "* **status**: The status of the movie (Released, To Be Released, etc.)\n",
    "* **tagline**: The movie's tagline.\n",
    "* **title**: The official title of the movie.\n",
    "* **video**: Indicates whether there is a video of the movie in TMDb.\n",
    "* **vote_average**: The average rating of the movie, on a 0-10 scale.\n",
    "* **vote_count**: The number of votes by users, as counted by TMDb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6b418588e3f9139f74cb3a9546f5dca49729579b",
    "id": "2Clue5Z_Cgbc"
   },
   "source": [
    "# Imports\n",
    "\n",
    "First, let's make sure to import Pandas and NumPy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-27T10:09:32.802462Z",
     "start_time": "2021-09-27T10:09:30.598648Z"
    },
    "id": "A0K3Aw3eAf2B"
   },
   "outputs": [],
   "source": [
    "#data analysis libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "#visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFlHk-e0BZME"
   },
   "source": [
    "# Exercise 1: Loading, preprocessing and cleaning the data (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGJnqtQD1BOs"
   },
   "source": [
    "Read the movie dataset from the following URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-27T10:09:34.147572Z",
     "start_time": "2021-09-27T10:09:34.145160Z"
    },
    "id": "sQb46KRIAkOB"
   },
   "outputs": [],
   "source": [
    "url = 'http://courses.compute.dtu.dk/02807/2021/projects/project1/movies_metadata.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQrm5PleClKz"
   },
   "source": [
    "## Part 1: Reading and preprocessing the data (10 pts)\n",
    "\n",
    "Pandas infers a data type for raw data from a `.csv`, defaulting to string type when no other `dtype` could be established. For example, the `genres` column in our dataset is read as a string with a *stringified* list of dictionaries as cell content. \n",
    "\n",
    "Some preprocessing steps are therefore needed, to convert the columns into their proper data types.\n",
    "\n",
    "Write a function `load_movies_data()` that reads the URL into a Pandas DataFrame and preprocesses its columns to ensure that:\n",
    "\n",
    "1. Data in the `release_date` column consists of Pandas `Timestamp` objects, except for missing values. For example, executing a code cell with `df.release_date[0]` should display the output `Timestamp('1995-10-30 00:00:00')`.\n",
    "\n",
    "2. Data in `belongs_to_collection` consists of dictionaries, except for missing values.\n",
    "\n",
    "3. Data in `genres`, `production_companies` and `production_countries` consists of lists of dictionaries, except for missing values. \n",
    "\n",
    "For example, executing a code cell with `df.genres[0]` should display the output \n",
    "```\n",
    "[{'id': 16, 'name': 'Animation'},\n",
    " {'id': 35, 'name': 'Comedy'},\n",
    " {'id': 10751, 'name': 'Family'}]\n",
    "```\n",
    "which is a list type, not a string. The elements of the list are dictionaries (executing `df.genres[0][0]['name']` returns `'Animation'`). \n",
    "\n",
    "**Hint**: for items 2 and 3, you should use `ast.literal_eval`.\n",
    "\n",
    "These conversions can be performed using Pandas' built-in functions and/or calling Pandas' `apply()` with appropriate arguments. Avoid explicit looping. You'll be asked below to time the loading and preprocessing step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "2mC5hjGXJTZ0"
   },
   "outputs": [],
   "source": [
    "def load_movies_data(url):\n",
    "    #import the movie ratings dataset and test CSV files\n",
    "    df = pd.read_csv(url)\n",
    "    ## fill NaN values with None for easier handling\n",
    "    df = df.fillna('None')\n",
    "    ## convert column release_date from str to timestamp format\n",
    "    df['release_date'] = pd.to_datetime(df['release_date'], format='%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "    ## convert selected columns to dict format \n",
    "    cols = ['belongs_to_collection','genres','production_companies','production_countries']\n",
    "    for col in cols:\n",
    "        df[col] = [eval(x) for x in df[col]]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZvSPPdzfScL"
   },
   "source": [
    "Now call `load_movies_data()` and load the data into a DataFrame `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4-eOSqlhD-8J"
   },
   "outputs": [],
   "source": [
    "df = load_movies_data(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bnelY1bZzj-J"
   },
   "source": [
    "Display the DataFrame. You should check that it looks correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tP6zpeLxu4J-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45466 entries, 0 to 45465\n",
      "Data columns (total 24 columns):\n",
      " #   Column                 Non-Null Count  Dtype         \n",
      "---  ------                 --------------  -----         \n",
      " 0   adult                  45466 non-null  object        \n",
      " 1   belongs_to_collection  4494 non-null   object        \n",
      " 2   budget                 45466 non-null  object        \n",
      " 3   genres                 45466 non-null  object        \n",
      " 4   homepage               45466 non-null  object        \n",
      " 5   id                     45466 non-null  object        \n",
      " 6   imdb_id                45466 non-null  object        \n",
      " 7   original_language      45466 non-null  object        \n",
      " 8   original_title         45466 non-null  object        \n",
      " 9   overview               45466 non-null  object        \n",
      " 10  popularity             45466 non-null  object        \n",
      " 11  poster_path            45466 non-null  object        \n",
      " 12  production_companies   45463 non-null  object        \n",
      " 13  production_countries   45463 non-null  object        \n",
      " 14  release_date           45376 non-null  datetime64[ns]\n",
      " 15  revenue                45466 non-null  object        \n",
      " 16  runtime                45466 non-null  object        \n",
      " 17  spoken_languages       45466 non-null  object        \n",
      " 18  status                 45466 non-null  object        \n",
      " 19  tagline                45466 non-null  object        \n",
      " 20  title                  45466 non-null  object        \n",
      " 21  video                  45466 non-null  object        \n",
      " 22  vote_average           45466 non-null  object        \n",
      " 23  vote_count             45466 non-null  object        \n",
      "dtypes: datetime64[ns](1), object(23)\n",
      "memory usage: 8.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PuzjOV7ffbpR"
   },
   "source": [
    "## Part 2: Timing your function (2.5 pts)\n",
    "\n",
    "Time the performance of your function. To get the points for this part, the time reported below must not exceed 40 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "aIYWL17bO2Ih"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-103-c8f0c40d7de2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mload_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'timeit'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'-o -r 3 load_movies_data(url)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Time (s):\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mload_time\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[1;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[0;32m   2315\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_ns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2316\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2317\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2318\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<C:\\Users\\hanlu\\anaconda3\\lib\\site-packages\\decorator.py:decorator-gen-61>\u001b[0m in \u001b[0;36mtimeit\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\magics\\execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1162\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1164\u001b[1;33m         \u001b[0mall_runs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1165\u001b[0m         \u001b[0mbest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_runs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnumber\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1166\u001b[0m         \u001b[0mworst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_runs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnumber\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\timeit.py\u001b[0m in \u001b[0;36mrepeat\u001b[1;34m(self, repeat, number)\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m             \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\magics\\execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[1;34m(self, number)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m             \u001b[0mtiming\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[1;34m(_it, _timer)\u001b[0m\n",
      "\u001b[1;32m<ipython-input-98-c22689c8cb96>\u001b[0m in \u001b[0;36mload_movies_data\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_movies_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m#import the movie ratings dataset and test CSV files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;31m## fill NaN values with None for easier handling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'None'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m             \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"encoding_errors\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"strict\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m         )\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    612\u001b[0m         \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m         \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 614\u001b[1;33m         \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    615\u001b[0m     )\n\u001b[0;32m    616\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36m_get_filepath_or_buffer\u001b[1;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[0;32m    315\u001b[0m                 \u001b[1;31m# Override compression based on Content-Encoding header\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m                 \u001b[0mcompression\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"method\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"gzip\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m             \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    318\u001b[0m         return IOArgs(\n\u001b[0;32m    319\u001b[0m             \u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    468\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 470\u001b[1;33m                     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    471\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_close_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_safe_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    618\u001b[0m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mamt\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 620\u001b[1;33m             \u001b[0mchunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAXAMOUNT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    621\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "load_time = %timeit -o -r 3 load_movies_data(url)\n",
    "print(\"Time (s):\", load_time.best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7sVQeTkLYI2b"
   },
   "source": [
    "## Part 3: Cleaning the data (2.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ydbZ-KFYlRX"
   },
   "source": [
    "Filter/drop all rows in `df` meeting any of these conditions:\n",
    "* The `adult` value is not `'False'`\n",
    "* The `vote_count` value is missing\n",
    "* The `vote_average` value is missing\n",
    "\n",
    "Do not loop over rows to perform these checks. Use Pandas' built-in functionality to do so. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Qtb7AaaVY5nm"
   },
   "outputs": [],
   "source": [
    "## drop rows where adult value is not 'False'\n",
    "df = df[(df['adult'] == False) & (df['vote_count'] != 'None') & (df['vote_average'] != 'None')].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adult</th>\n",
       "      <th>belongs_to_collection</th>\n",
       "      <th>budget</th>\n",
       "      <th>genres</th>\n",
       "      <th>homepage</th>\n",
       "      <th>id</th>\n",
       "      <th>imdb_id</th>\n",
       "      <th>original_language</th>\n",
       "      <th>original_title</th>\n",
       "      <th>overview</th>\n",
       "      <th>...</th>\n",
       "      <th>release_date</th>\n",
       "      <th>revenue</th>\n",
       "      <th>runtime</th>\n",
       "      <th>spoken_languages</th>\n",
       "      <th>status</th>\n",
       "      <th>tagline</th>\n",
       "      <th>title</th>\n",
       "      <th>video</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>vote_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [adult, belongs_to_collection, budget, genres, homepage, id, imdb_id, original_language, original_title, overview, popularity, poster_path, production_companies, production_countries, release_date, revenue, runtime, spoken_languages, status, tagline, title, video, vote_average, vote_count]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 24 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mT8jdILukiBF"
   },
   "source": [
    "# Exercise 2: Computing IMDb's ratings (35 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84SE_eMzkyC1"
   },
   "source": [
    "The Top Rated 250 titles in IMDb are calculated using [a formula](https://help.imdb.com/article/imdb/track-movies-tv/ratings-faq/G67Y87TFYYP6TWAV#calculatetop) that takes into account the number of votes that a title has received, the minimum votes required to be on the list, and the mean vote for all titles. The rating for a title is given as follows:\n",
    "\n",
    "$$ \\text{weighted rating } = \\left(\\frac{v}{v+m} \\cdot R\\right) + \\left(\\frac{m}{v+m} \\cdot C\\right)$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$m$ = the minimum number of votes required to be listed in the Top Rated ranking. We'll let $m=1000$.\n",
    "\n",
    "$v$ = the number of votes received by the title (the title's **`vote_count`** value)\n",
    "\n",
    "$R$ = the average rating for the title (the title's **`vote_average`** value)\n",
    "\n",
    "$C$ = the mean vote across the whole list (the mean over the **`vote_average`** column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85J4HDO2rieA"
   },
   "source": [
    "We are going to compute the ratings for movies that could be listed in IMDb's Top Rated 250 ranking.  We want to do this as efficiently as possible. As a baseline for benchmarking, we'll use an approach that explicitly loops and indexes over the rows of the dataset and computes the weighted rating for the corresponding movie (if the movie has more than 1000 votes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-27T10:27:52.905697Z",
     "start_time": "2021-09-27T10:27:52.800529Z"
    },
    "id": "MyYhLdBksTec"
   },
   "outputs": [],
   "source": [
    "C = df['vote_average'].mean()\n",
    "m = 1000\n",
    "\n",
    "def weighted_rating(row):\n",
    "    if row['vote_count'] > m:\n",
    "        v = row['vote_count']\n",
    "        R = row['vote_average']\n",
    "        return (v/(v+m) * R) + (m/(v+m) * C)\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def weighted_rating_loop(df):\n",
    "    rating_list = []\n",
    "    for i in range(len(df)):\n",
    "        rating = weighted_rating(df.iloc[i])\n",
    "        rating_list.append(rating)\n",
    "    df['imdb_rating'] = rating_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JIsei1w4r9Jz"
   },
   "outputs": [],
   "source": [
    "weighted_rating_loop(df)\n",
    "\n",
    "columns_to_show = ['id', 'original_title'] + \\\n",
    "                  list(df.columns[df.columns.str.startswith('imdb_rating')])\n",
    "df[columns_to_show].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JdlffrsJyHvI"
   },
   "source": [
    "Let's look at the average performance of this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "su__dMpZyX5W"
   },
   "outputs": [],
   "source": [
    "basic_time = %timeit -r 3 -o weighted_rating_loop(df)\n",
    "print(\"Best time:\", basic_time.best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HPjyKJ2zzPoQ"
   },
   "source": [
    "In the remaining parts of the exercise, you are going to be asked to come up with alternative ways to compute the ratings, using various methodologies. Let's create a score board to keep track of performance. Here's a description of the rows:\n",
    "\n",
    "*   **Best single run time (s)**:  The best time used by your solution, in seconds.\n",
    "*   **Marginal performance improvement**: The time improvement of your current solution over its immediately preceding solution. Given by: $\\frac{\\text{best single run time (s) of previous solution}}{\\text{best single run time (s) of current solution}}$\n",
    "*   **Performance improvement over basic looping**:  The time improvement over our baseline solution. Given by: $\\frac{\\text{best single run time (s) of weighted_rating_loop}}{\\text{best single run time (s) of current solution}}$\n",
    "*   **Best single run time (s, teacher)**: The time of a solution provided by the teacher. \n",
    "*   **Marginal performance improvement (teacher)**: The time improvement of the teacher's solution over its immediately preceding solution. \n",
    "*   **Performance improvement over basic looping (teacher)**:  The teacher's solution improvement over the baseline solution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-27T11:03:14.455052Z",
     "start_time": "2021-09-27T11:03:14.442251Z"
    },
    "id": "fV-xfrs53NBF"
   },
   "outputs": [],
   "source": [
    "timing_data = {\n",
    "    'Best single run time (s)': [basic_time.best, np.nan, np.nan, np.nan,np.nan],\n",
    "    'Marginal performance improvement': [np.nan,np.nan, np.nan, np.nan,np.nan],\n",
    "    'Performance improvement over basic looping': np.nan,\n",
    "    'Best single run time (s, teacher)': [9.37, 3.87, 0.562, 0.0054, 0.00084],\n",
    "    'Marginal performance improvement (teacher)': [np.nan, 'x2.42', 'x6.88', 'x103.8', 'x6.45'],\n",
    "    'Performance improvement over basic looping (teacher)': [np.nan, 'x2.42', 'x16.69', 'x1732.17','x11172.98']\n",
    "}\n",
    "\n",
    "indices = ['Basic looping', 'Iterrows looping', 'apply()', 'Pandas vectorisation', 'NumPy vectorisation']\n",
    "timings = pd.DataFrame(timing_data, index=indices)\n",
    "timings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfD0eUlHgz3l"
   },
   "source": [
    "**The grading for the following parts works as follows.**\n",
    "\n",
    "Let $m$ be the marginal performance improvement for the teacher's solution over basic looping, and let $m'$ be the marginal performance improvement for your solution over `basic_time`. If a part gives $n$ points, then you will get the $n$ points if $m' \\geq 0.4 m$, and 0 points otherwise.\n",
    "\n",
    "You don't get extra points for performing faster than the teacher's solution. But this is of course possible and you should feel free to optimise away as much as you want!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkiLI2QT7oNY"
   },
   "source": [
    "## Part 1: Looping with `iterrows` (2.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmZIqPIf743q"
   },
   "source": [
    "Define a function `weighted_rating_iterrows(df)` that computes the ratings by looping over rows with the built-in iterator `iterrows`, and stores the results in a new column of the DataFrame called called `imdb_rating_iter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m4dTxRfW6Gc4"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsvOtN6y8Ycg"
   },
   "source": [
    "Call the function and make sure that it works as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sLKzzVP86bza"
   },
   "outputs": [],
   "source": [
    "weighted_rating_iterrows(df)\n",
    "\n",
    "pd.testing.assert_series_equal(\n",
    "    df.imdb_rating, df.imdb_rating_iter, check_names=False\n",
    ")\n",
    "\n",
    "columns_to_show = ['id', 'original_title'] + \\\n",
    "                  list(df.columns[df.columns.str.startswith('imdb_rating')])\n",
    "df[columns_to_show].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AC-m0iR48jM6"
   },
   "source": [
    "Time the performance of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9y3BWusJ8y70"
   },
   "outputs": [],
   "source": [
    "iterrows_time = %timeit -r 3 -o weighted_rating_iterrows(df)\n",
    "print(\"Best time:\", iterrows_time.best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HexMOOCB82e_"
   },
   "source": [
    "Update the score board with the best time, marginal and overall performance change you have obtained. Display the updated table below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_SQRS_gdJCbM"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-FhuhB99em4"
   },
   "source": [
    "## Part 2: Using `apply()`. (5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZ294FYn9qhO"
   },
   "source": [
    "Define a function `weighted_rating_apply(df)` that computes the ratings using Pandas' `apply()` function, and stores the results in a new column of the DataFrame called `imdb_rating_apply`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gk3SdbQs8ghG"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBgP1e1uCMlF"
   },
   "source": [
    "Call the function and make sure that it works as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wUaYXC_RCMlK"
   },
   "outputs": [],
   "source": [
    "weighted_rating_apply(df)\n",
    "\n",
    "pd.testing.assert_series_equal(\n",
    "    df.imdb_rating, df.imdb_rating_apply, check_names=False\n",
    ")\n",
    "\n",
    "columns_to_show = ['id', 'original_title'] + \\\n",
    "                  list(df.columns[df.columns.str.startswith('imdb_rating')])\n",
    "df[columns_to_show].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C9jjZI14CMlO"
   },
   "source": [
    "Time the performance of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jqggBfTMCMlP"
   },
   "outputs": [],
   "source": [
    "apply_time = %timeit -r 3 -o weighted_rating_apply(df)\n",
    "print(\"Best time:\", apply_time.best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DzMnzf4CMlS"
   },
   "source": [
    "Update the score board with the best time, marginal and overall performance change you have obtained. Display the updated table below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-4RSuVOYV0JZ"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZHPgFMsED_6"
   },
   "source": [
    "## Part 3: Vectorised solution with Pandas (12.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CI7zQxGNGZnX"
   },
   "source": [
    "Let's find a vectorised solution using Pandas. You have to define a function `weighted_rating_pandas(df)` that computes the ratings in a vectorised way and stores them in a new column of the DataFrame called `imdb_rating_pandas`. Use Pandas operations only: don't transform your data into NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WqIXsMGFGZnY"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKVGubslGZna"
   },
   "source": [
    "Call the function and make sure it works as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yXf4KAj1GZna"
   },
   "outputs": [],
   "source": [
    "weighted_rating_pandas(df)\n",
    "\n",
    "pd.testing.assert_series_equal(\n",
    "    df.imdb_rating, df.imdb_rating_pandas, check_names=False\n",
    ")\n",
    "\n",
    "columns_to_show = ['id', 'original_title'] + \\\n",
    "                  list(df.columns[df.columns.str.startswith('imdb_rating')])\n",
    "df[columns_to_show].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scNFR7X8GZnc"
   },
   "source": [
    "Time the performance of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TbhwZZ0mGZnf"
   },
   "outputs": [],
   "source": [
    "pandas_time = %timeit -r 3 -o weighted_rating_pandas(df)\n",
    "print(\"Best time:\", pandas_time.best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xsqz0kUJGZnh"
   },
   "source": [
    "Update the score board with the best time, marginal and overall performance change you have obtained. Display the updated table below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SxCI0nUXV3Lk"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HvwaE7pIjzEa"
   },
   "source": [
    "Time to reflect on your solution. Do the following: \n",
    " \n",
    "* Explain in words what your function does and why it is a vectorised solution. In particular, break down each step involving ufuncs, broadcasting and other vectorized calls. \n",
    "    * If your explanation has major errors, we will substract points for this part.\n",
    " \n",
    "* Display profiler output and give an analysis of what you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bQHB_JqeV52E"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DngbQA2eV770"
   },
   "source": [
    "*Your explanation goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLwJEzATEKwi"
   },
   "source": [
    "## Part 4: Vectorised solution with NumPy (12.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GrAZ4OI2EPk8"
   },
   "source": [
    "Let's find a vectorised solution that uses NumPy to speed up the calculations. You have to define a function `weighted_rating_numpy(df)` that computes the ratings in a vectorised way and stores them in a new column of the DataFrame called `imdb_rating_numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9sJlkDYAzQr8"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2pAc_GEEudy"
   },
   "source": [
    "Call the function and make sure it works as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IALbcsKO2RNt"
   },
   "outputs": [],
   "source": [
    "weighted_rating_numpy(df)\n",
    "\n",
    "pd.testing.assert_series_equal(\n",
    "    df.imdb_rating, df.imdb_rating_numpy, check_names=False\n",
    ")\n",
    "\n",
    "columns_to_show = ['id', 'original_title'] + \\\n",
    "                  list(df.columns[df.columns.str.startswith('imdb_rating')])\n",
    "df[columns_to_show].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0ebRPwiFAfo"
   },
   "source": [
    "Time the best performance of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eWe52oAQ6l8R"
   },
   "outputs": [],
   "source": [
    "numpy_time = %timeit -r 3 -o weighted_rating_numpy(df)\n",
    "print(\"Best time:\", numpy_time.best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TB7tkhUDFAfr"
   },
   "source": [
    "Update the score board with the best time, marginal and overall performance change you have obtained. Display the updated table below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J8MBiqnvushL"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8RSyEDTvA-QQ"
   },
   "source": [
    "Time to reflect on your solution. Do the following: \n",
    " \n",
    "* Explain in words what your function does and why it is a vectorised solution. In particular, break down each step involving ufuncs, broadcasting and other vectorized calls. \n",
    "    * If your explanation has major errors, we will substract points for this part.\n",
    "    * If applicable, you may refer back to the explanation you gave for `weighted_rating_pandas`.\n",
    " \n",
    "* Display profiler output and give an analysis of what you see.\n",
    "    * Contrast your findings with those from `weighted_rating_pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FV8aYhv5WEqA"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7lgegwDWFTt"
   },
   "source": [
    "*Your explanation goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rcGeZKuIRHe7"
   },
   "source": [
    "## Part 5: Find out the top 25 titles (2.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-IE_GfsRMOG"
   },
   "source": [
    "What are the top 25 titles? Now that we have the IMDb ratings conveniently stored in a column, display the top 25 titles, together with their IMDb rating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "svC-x1ZcRNJ4"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Ca4XaEtqvwt"
   },
   "source": [
    "# Exercise 3: Predicting the genre of movies (40 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irkcmAfTHTzR"
   },
   "source": [
    "In this exercise, you'll be asked to create a number of features and use them to predict whether a movie is a science fiction movie or not. \n",
    "For this classification task, we'll work with a different part of the movies dataset, which contains more information for each movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-27T12:28:57.151965Z",
     "start_time": "2021-09-27T12:28:57.145452Z"
    },
    "id": "cs0dJMEG1PLL"
   },
   "outputs": [],
   "source": [
    "train_url = 'http://courses.compute.dtu.dk/02807/2021/projects/project1/train.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIcpNS2GCXUi"
   },
   "source": [
    "You'll try to predict whether a movie is a science fiction movie based on the other associated genres for the movie, the people and companies involved in making it, as well as its release date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkN5bCjoCPW9"
   },
   "source": [
    "## Part 1: Adding binary features for genres (15 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XA00jck22BKq"
   },
   "source": [
    "As in Exercise 1, the data on several columns is in a stringified format. Pre-process the following columns appropriately, as you did with the `genres` column in Part 1 of Exercise 1.\n",
    "```\n",
    "'belongs_to_collection', 'genres', 'production_companies','production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew'\n",
    "```\n",
    " \n",
    "Don't loop explicitly over the rows to perform this preprocessing. Your dataframe should be named `train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tHi0Ejxb2bKM"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z57lsGZ5IL7b"
   },
   "source": [
    "Looking at the 'genres' column, you can see that movies have a varying number of associated genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "amzV_VzSE0VL"
   },
   "outputs": [],
   "source": [
    "# this will work only if you've already preprocessed the genres' column into lists of dicts\n",
    "for i, v in enumerate(train.genres.head()):\n",
    "    print(i, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPDjPDHlInOC"
   },
   "source": [
    "Count the number of movies that have $n$ associated genres, for each $n$ in the dataset. If a movie has no associated genres, assign it the number 0. \n",
    "\n",
    "You have to use Pandas built-in functions only (no explicit looping). For example, you could use `apply()` with an appropriate function to apply to each row. \n",
    "\n",
    "Once you have the counts, visualise them as a bar chart, with one bar per possible number of associated genres, and the height of the bar representing the number of movies with that number of genres.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6cHqZJ8iImh1"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqA9UQ3rJyBQ"
   },
   "source": [
    "Let's create our binary features next. Complete the following steps:\n",
    " \n",
    "1.   Transform the `genres` column by replacing its current entries with the list of names of genres occurring in the entries.  For example, the entry \n",
    "```\n",
    "[{'id': 10749, 'name': 'Romance'}, {'id': 35, 'name': 'Comedy'}]\n",
    "```\n",
    "should be transformed into:\n",
    "```\n",
    "['Romance','Comedy']\n",
    "```\n",
    "Empty entries should be transformed into the empty list `[]`.\n",
    "\n",
    "2. Create a separate column (in `train`) for each of the 20 genres, with name `genres_(nameofgenre)` (e.g. `genres_Comedy`). A movie should have a 1 on a genres column if the genre is one of the associated genres for that movie, and a 0 otherwise.\n",
    "    * To get started, consider what operations create a data frame with dimensions as `train` and columns as specified here, based on the list output from step 1. Then combine this data frame with `train`.\n",
    "\n",
    "You have to use Pandas built-in functions only (no explicit looping)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EVXMvDv3Mk1t"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nN65zGfLnSe-"
   },
   "source": [
    "Visualise the number of movies per top 20 genre with a chart of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wEjJJkMZh9mF"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M79tzVdMDfXL"
   },
   "source": [
    "## Part 2: Adding more binary features (5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iZngZ3ZnAsG"
   },
   "source": [
    "You've now extracted binary features for all genres associated with a movie. But there's other information that we could use to base our predictions on. \n",
    "\n",
    "The `genres` column is just one out of several columns containing lists of dictionaries as entries. For example, the `production_companies` column also contains lists of dictionaries, providing names of the companies producing the movie. As you just did with genres, add new columns for:\n",
    " \n",
    "1.   The names of the 30 most common production companies\n",
    "2.   The names of the 30 most common production countries\n",
    "3.   The names of the 30 most common actors (`cast` column) \n",
    "4.   The names of the 30 most common crew members\n",
    "5.   The names of the 30 most common keywords\n",
    " \n",
    "We recommend you generalize the functionality implemented in the previous question (e.g. to other columns and to restrict to top 30 most common values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9sO8Tsqduatn"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjFFSEJs93yJ"
   },
   "source": [
    "Check the result. You should now have a much wider table, with the new columns consisting of binary features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IyORi5V27Mg0"
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "joEcHikdD5SY"
   },
   "source": [
    " ## Part 3: Adding numerical date features (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nliC-9woDYd_"
   },
   "source": [
    "Next, we'll create some features based on the release date information. Create a new column storing the value for each of the following  aspects of a release date:\n",
    " \n",
    "```\n",
    "['year', 'weekday', 'month', 'weekofyear', 'day', 'quarter']\n",
    "```\n",
    " \n",
    "As usual, don't iterate explicitly to create these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-K5u197bDX1A"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KWLi9-BW97VX"
   },
   "source": [
    "Next, we'll drop the columns that will not be used for learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-27T15:27:05.446343Z",
     "start_time": "2021-09-27T15:27:05.426250Z"
    },
    "id": "MbycSRqu8dR5"
   },
   "outputs": [],
   "source": [
    "train = train.drop(['id', 'homepage', 'original_language',\n",
    "                    'title', 'imdb_id','crew', 'poster_path', \n",
    "                    'release_date', 'status', 'belongs_to_collection',\n",
    "                    'Keywords', 'original_title', 'overview',\n",
    "                    'production_companies', 'production_countries', \n",
    "                    'spoken_languages', 'tagline', 'cast','genres'], \n",
    "                   axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOlV7ho4-OR9"
   },
   "source": [
    "Lastly, drop any rows with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "asrzvNYy-NO4"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tV_ANmOH0rQJ"
   },
   "source": [
    "## Part 4: Prediction (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Y1XUwNX0rQL"
   },
   "source": [
    "Let's load the necessary `sklearn` libraries and prepare the training data for learning. Recall that your goal is to predict whether a movie has science fiction as an associated genre. So you're dealing with a binary classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Qpz2Cd60rQM"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWzqddf90rQR"
   },
   "source": [
    "Use `sklearn` to prepare the training and test sets, setting aside 15% of the data for testing. Call the training input features, training labels, test input features and test labels as follows:\n",
    "\n",
    "```\n",
    "x_train, x_test, y_train, y_test\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dDLDQWPl0rQS"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKaGZrSuLg1K"
   },
   "source": [
    "Feature scaling is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance). Run the following code to feature scale your input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "by0c0S7s0rQY"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler  \n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(x_train)  \n",
    "x_train = scaler.transform(x_train) \n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxX0ugDuNR8o"
   },
   "source": [
    "Check that the shape of your data looks correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5tJHqw2M0rQf"
   },
   "outputs": [],
   "source": [
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90SIHxqzN3Ht"
   },
   "source": [
    "Train a classifier of your choice. Then report results:\n",
    "* Display the confusion matrix over the test set in absolute numbers.\n",
    "    * These numbers reflect number of true positives, true negatives, false positives and false negatives.\n",
    "* Display a normalized confusion matrix over the test set, so [sensitivity and specificity](https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers) can be read from the diagonal (off-diagonal will contain type I and type II error rates).\n",
    "    * Note that sensitivity is recall for the positive class (1), whereas specificity is recall for the negative class (0).\n",
    "* State **in free-text** the sensitivity and specificity of your classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lIg8WIm3NiKN"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBkWn4WXA-QS"
   },
   "source": [
    "*Your explanation here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oP8IaWa9A-QS"
   },
   "source": [
    "## Part 5: Prediction with less leakage (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKIGA3BOA-QS"
   },
   "source": [
    "From Wikipedia, [data leakage](https://en.wikipedia.org/wiki/Leakage_(machine_learning)) is the use of information in the model training process which would not be expected to be available at prediction time, causing the predictive scores (metrics) to overestimate the model's utility when run in a production environment.\n",
    "\n",
    "Feature or column-wise leakage is caused by the inclusion of columns which are one of the following: a duplicate label, a proxy for the label, or the label itself\n",
    "\n",
    "Considering we're doing binary classification of whether a movie is science fiction, identify the most prominent cause of feature leakage among the features added during Exercise 3. \n",
    "\n",
    "1) **Argue** for your choice, 2) train a classifier not subject to this feature leakage (it's OK to create a new train/test split) and 3) **report the results** as you did in Part 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mb7FTGgTA-QS"
   },
   "source": [
    "*Your explanation here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-27T14:01:15.648444Z",
     "start_time": "2021-09-27T14:01:15.643363Z"
    },
    "id": "9CzDjF_pA-QS"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hc4EOB-gb5pp"
   },
   "source": [
    "# Exercise 4: Basic movie recommendation system (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PqirpLgVdN0"
   },
   "source": [
    "In this exercise, you'll build a simple movie recommendation system.  The system will take a movie as input and recommend a list of similar movies. In order to recommend similar movies, you will use the correlation between the ratings of movies as a similarity metric. We'll use Pearson's correlation. \n",
    " \n",
    "The data for this exercise is available in the following URLs. It contains basic info about movies, as well as ratings provided by several users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-27T14:49:08.552678Z",
     "start_time": "2021-09-27T14:49:08.547286Z"
    },
    "id": "HOHERjo8cLQo"
   },
   "outputs": [],
   "source": [
    "url1 = 'http://courses.compute.dtu.dk/02807/2021/projects/project1/ratings.csv'\n",
    "url2 = 'http://courses.compute.dtu.dk/02807/2021/projects/project1/movies.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOXKd22xXgo2"
   },
   "source": [
    "## Part 1: Preparing the ratings data (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Isn8M65HXi3i"
   },
   "source": [
    "Read the data from these two URLs, and create a single dataframe from them, with the following columns:\n",
    "\n",
    "| userId | movieId | rating | timestamp | title | genres |\n",
    "|--------|---------|--------|-----------|-------|--------|\n",
    "|        |         |        |           |       |        |\n",
    "\n",
    "Call the dataframe `movie_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwhdEsGUn1zz"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFGc9VSzadHi"
   },
   "source": [
    "To find the correlation between the ratings of movies, create a dataframe where each column is a movie name and each row contains the rating assigned by a specific user to that movie. \n",
    "\n",
    "You'll notice that this dataframe has many NaN values, since each movie is not rated by every user. Call the dataframe `user_ratings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eTrrVPsrfn1Q"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hy61Luqyu2Te"
   },
   "source": [
    "## Part 2: Finding the most similar movies (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7FBDshDd1j7"
   },
   "source": [
    "Each column contains all the user ratings for a particular movie. Let's take the user ratings for the movie Toy Story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6e-9WXAro_4-"
   },
   "outputs": [],
   "source": [
    "toystory_ratings = user_ratings['Toy Story (1995)']\n",
    "toystory_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fljIQkJmpCJD"
   },
   "source": [
    "Next, find the correlation between the user ratings for Toy Story and the user ratings of all other movies. \n",
    " \n",
    "More specifically, create a dataframe that contains two columns, called `title` and `Correlation`. Each row should contain a movie title $x$, followed by the pairwise correlation between the column of ratings for Toy Story and the column of ratings for $x$.  Drop any rows with null values, and display the resulting dataframe.\n",
    " \n",
    "Use built-in functions to compute correlations and avoid explicit loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_yuX1U0Xh0nk"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aji7cUw-r-JG"
   },
   "source": [
    "Sort the movies by descending order of correlation to find out highly correlated movies at the top. Display the 5 most highly correlated movies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FU0Zm9fjg4BD"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qrvD_u63sVdD"
   },
   "source": [
    "If you computed correlations correctly, you will find that the recommended movies are not very well known. We can generate more popular recommendations by finding highly correlated movies that have a sensible number of ratings. \n",
    " \n",
    "Add a column to your correlation table, called `rating_counts`, which stores the number of ratings received by each movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WvMKUmBFg-Ow"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_20nfFvt-7y"
   },
   "source": [
    "Now find the 5 movies with the highest correlation with Toy Story, which have strictly more than 100 ratings. Display the result below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KvE3c2lMhHNz"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "7sVQeTkLYI2b"
   ],
   "include_colab_link": true,
   "name": "02807_Project_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "362px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
