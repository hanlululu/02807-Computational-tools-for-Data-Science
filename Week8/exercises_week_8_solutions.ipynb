{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "exercises_week_8_solutions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "384px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hanlululu/02807-Computational-tools-for-Data-Science/blob/main/Week8/exercises_week_8_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlMcbNUbMqnF"
      },
      "source": [
        "# 02807 - Week 8 Exercises:  More Spark DataFrame API\n",
        "\n",
        "In this exercise session you will continue learning PySpark running it in local mode.\n",
        "\n",
        "These exercises are inspired by [Tiziano Piccardi](http://piccardi.me/) and [this notebook](https://github.com/epfl-ada/2019/blob/master/Tutorials/04%20-%20Scaling%20Up/PySpark.ipynb) (which contains partial solutions). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXQzA01OS_yQ"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbYZoVVWOZA5"
      },
      "source": [
        "Setup spark for running in local mode using these instructions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-10-24T19:26:49.907614Z",
          "start_time": "2021-10-24T19:26:48.993636Z"
        },
        "id": "dhzk3GE6S9RC",
        "outputId": "92d9171a-9e32-481b-ab5a-8c7ea20322e1"
      },
      "source": [
        "!pip install pyspark\n",
        "\n",
        "# Instructions on p. 20 Learning Spark, 2nd ed.\n",
        "# Here's a quick-guide, googling may also be required\n",
        "# 1) Install pyspark via conda/pip\n",
        "#          pyspark requires the JAVA_HOME environment variable is set.\n",
        "# 2) Install JDK 8 or 11, figure out the install location\n",
        "#          Suggest to use https://adoptopenjdk.net/\n",
        "# 3) Update the JAVA_HOME environment variable set programmatically below \n",
        "#    with your install location specifics\n",
        "\n",
        "# JAVA_HOME environment variable is set programatically below\n",
        "# but you must point it to your local install\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home/\"\n",
        "\n",
        "# If you get \"Job aborted due to stage failure\" and \n",
        "# \"Python worker failed to connect back.\" exceptions, \n",
        "# this should be solved by additionally setting these \n",
        "# environment variables\n",
        "\n",
        "# os.environ['PYSPARK_PYTHON'] = 'python'\n",
        "# os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
        "# os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'notebook'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /Users/martinholmjensen/miniforge3/envs/py38/lib/python3.8/site-packages (3.2.0)\r\n",
            "Requirement already satisfied: py4j==0.10.9.2 in /Users/martinholmjensen/miniforge3/envs/py38/lib/python3.8/site-packages (from pyspark) (0.10.9.2)\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-10-24T19:26:50.399409Z",
          "start_time": "2021-10-24T19:26:49.908924Z"
        },
        "id": "xgQaRx6rSaf9"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark import SparkContext, SparkConf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-10-24T19:26:56.315373Z",
          "start_time": "2021-10-24T19:26:50.400315Z"
        },
        "id": "7ft3VivrSagB",
        "outputId": "f79b26f3-ad16-4347-d443-cc7e3afd530a"
      },
      "source": [
        "# create the Spark session\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "spark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://csrls2.private.imm.dtu.dk:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x137b69790>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gewv-lKMSagI"
      },
      "source": [
        "# Vietnam War Dataset\n",
        "\n",
        "**Pres. Johnson**: _What do you think about this Vietnam thing? I’d like to hear you talk a little bit._\n",
        "\n",
        "**Sen. Russell**: _Well, frankly, Mr. President, it’s the damn worse mess that I ever saw, and I don’t like to brag and I never have been right many times in my life, but I knew that we were going to get into this sort of mess when we went in there._\n",
        "\n",
        "May 27, 1964\n",
        "\n",
        "![banner](https://raw.githubusercontent.com/epfl-ada/2019/c17af0d3c73f11cb083717b7408fedd86245dc4d/Tutorials/04%20-%20Scaling%20Up/img/banner.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skjUv84VSagJ"
      },
      "source": [
        "----\n",
        "\n",
        "The Vietnam War, also known as the Second Indochina War, and in Vietnam as the Resistance War Against America or simply the American War, was a conflict that occurred in Vietnam, Laos, and Cambodia from 1 November 1955 to the fall of Saigon on 30 April 1975. It was the second of the Indochina Wars and was officially fought between North Vietnam and the government of South Vietnam.\n",
        "\n",
        "**The dataset describes all the air force operation in during the Vietnam War.**\n",
        "\n",
        "**Bombing_Operations** [Get the dataset here](https://courses.compute.dtu.dk/02807/2021/lectures/week8/Bombing_Operations.json.gz)\n",
        "\n",
        "- AirCraft: _Aircraft model (example: EC-47)_\n",
        "- ContryFlyingMission: _Country_\n",
        "- MissionDate: _Date of the mission_\n",
        "- OperationSupported: _Supported War operation_ (example: [Operation Rolling Thunder](https://en.wikipedia.org/wiki/Operation_Rolling_Thunder))\n",
        "- PeriodOfDay: _Day or night_\n",
        "- TakeoffLocation: _Take off airport_\n",
        "- TimeOnTarget\n",
        "- WeaponType\n",
        "- WeaponsLoadedWeight\n",
        "\n",
        "**Aircraft_Glossary** [Get the dataset here](https://courses.compute.dtu.dk/02807/2021/lectures/week8/Aircraft_Glossary.json.gz)\n",
        "\n",
        "- AirCraft: _Aircraft model (example: EC-47)_\n",
        "- AirCraftName\n",
        "- AirCraftType\n",
        "\n",
        "**Dataset Information:**\n",
        "\n",
        "THOR is a painstakingly cultivated database of historic aerial bombings from World War I through Vietnam. THOR has already proven useful in finding unexploded ordnance in Southeast Asia and improving Air Force combat tactics:\n",
        "https://www.kaggle.com/usaf/vietnam-war-bombing-operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSWoULeWSagJ"
      },
      "source": [
        "Load the datasets (this may require a bit of patience)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-10-24T19:27:02.529754Z",
          "start_time": "2021-10-24T19:26:56.316435Z"
        },
        "id": "XLyVPuLXSagK"
      },
      "source": [
        "df_aircraft = spark.read.json('Aircraft_Glossary.json.gz')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2021-10-24T19:26:48.980Z"
        },
        "id": "cYV3-4xhk1KT"
      },
      "source": [
        "df_operations = spark.read.json('Bombing_Operations.json.gz')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MP2JmCeoSagM"
      },
      "source": [
        "Display the schemas and inspect some rows of `df_operations`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2021-10-24T19:26:48.981Z"
        },
        "id": "KpUT8viNSagM"
      },
      "source": [
        "# your code goes here\n",
        "df_aircraft.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2021-10-24T19:26:48.982Z"
        },
        "id": "6LJsbmlxSagO"
      },
      "source": [
        "# your code goes here\n",
        "df_operations.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2021-10-24T19:26:48.985Z"
        },
        "id": "yhAgYMegk1KU"
      },
      "source": [
        "# your code goes here\n",
        "df_operations.limit(6).toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3U8BrxSk1KU"
      },
      "source": [
        "How many bombing operations are described in `df_operations`?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2021-10-24T19:26:48.989Z"
        },
        "id": "hMAab2rJSagU"
      },
      "source": [
        "# your code goes here\n",
        "f\"A total of {df_operations.count()} operations are described\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkKWEEcLSagW"
      },
      "source": [
        "# Question 1: Which countries are involved and in how many missions? \n",
        "\n",
        "Keywords: `Dataframe API`, `SQL`, `group by`, `sort`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwPL1-P5SagW"
      },
      "source": [
        "The country associated with the operation is found in the `ContryFlyingMission` [sic] field. Display the result using `show`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2021-10-24T19:26:48.991Z"
        },
        "id": "BWcMVcgwSagX",
        "scrolled": true
      },
      "source": [
        "# your code goes here\n",
        "df_operations.groupBy('ContryFlyingMission').count().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wQNXQnOSaga"
      },
      "source": [
        "Plot a horizontal bar chart with the number of operations by country. Consider using a logarithmic scale due to the skewedness of per country operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2021-10-24T19:26:48.993Z"
        },
        "id": "4nVJdPLDSagb"
      },
      "source": [
        "# your code goes here\n",
        "df_operations_count = df_operations \\\n",
        "                        .groupBy('ContryFlyingMission') \\\n",
        "                        .count() \\\n",
        "                        .sort(F.asc('count')) \\\n",
        "                        .toPandas()\n",
        "\n",
        "_ = df_operations_count.set_index('ContryFlyingMission').plot.barh(figsize=(16, 4), log=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLan66cXSagf"
      },
      "source": [
        "# Question 2: Show the number of missions in time for each of the countries involved.\n",
        "\n",
        "Transform the dataframe to contain only relevant columns for this query. Observe that `MissionDate` field is of type string. You can use `F.to_date` to convert it to a `DateType`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2021-10-24T19:26:48.995Z"
        },
        "id": "BXNKt5PHSagf"
      },
      "source": [
        "# your code goes here\n",
        "df_operations_q2 = df_operations.select(F.col('ContryFlyingMission'), \n",
        "                                        F.to_date(F.col('MissionDate'), 'yyyy-MM-dd').alias('MissionDate'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da9IeKntSagh"
      },
      "source": [
        "Now compute the count of missions per country for each `MissionDate` and sort the dataframe by date (ascending)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2021-10-24T19:26:48.997Z"
        },
        "id": "TLza0bF9Sagh"
      },
      "source": [
        "# your code goes here\n",
        "df_operations_q2_counts = df_operations_q2 \\\n",
        "                            .groupBy(['ContryFlyingMission', 'MissionDate']) \\\n",
        "                            .count() \\\n",
        "                            .sort(F.asc('MissionDate'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAhOE55wSagj"
      },
      "source": [
        "Now we can plot the content with a different series for each country (using an appropriate plot type). To this end, you may use the pattern from \"Slides Week 4\" where we did a scatter plot colored by country."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2021-10-24T19:26:48.999Z"
        },
        "id": "E5KH0aHvk1KW"
      },
      "source": [
        "# your code goes here\n",
        "df_operations_q2_counts_pd = df_operations_q2_counts.toPandas()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(16,4))\n",
        "for label, df in df_operations_q2_counts_pd.groupby(by='ContryFlyingMission'):\n",
        "    ax.plot(df['MissionDate'], df['count'], label=label)\n",
        "\n",
        "_ = ax.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEMzTwxGSagp"
      },
      "source": [
        "We can observe how South Vietnam increased its missions starting from 1970. The drop in 1973 is motivated by the [Paris Peace Accords](https://en.wikipedia.org/wiki/Paris_Peace_Accords) that took place on January 27th, 1973, to establish peace in Vietnam and end the war."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du9gqM_8k1KW"
      },
      "source": [
        "The plot looks a bit erratic, as the count is computed for each date. We can alleviate this by instead having the x-axis be months, and the y-axis be the count of operations in that month. Recompute such a dataframe and plot it once more. To this end, `F.trunc` will come in handy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2021-10-24T19:26:49.001Z"
        },
        "id": "lFzgxBLjk1KW"
      },
      "source": [
        "# your code goes here\n",
        "df_operations_q2_counts_months_pd = \\\n",
        "    df_operations_q2 \\\n",
        "        .withColumn('MissionYearMonth', F.trunc('MissionDate', 'mm')) \\\n",
        "        .groupBy(['ContryFlyingMission', 'MissionYearMonth']) \\\n",
        "        .count() \\\n",
        "        .sort(F.asc('MissionYearMonth')) \\\n",
        "        .toPandas()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(16,4))\n",
        "for label, df in df_operations_q2_counts_months_pd.groupby(by='ContryFlyingMission'):\n",
        "    ax.plot(df['MissionYearMonth'], df['count'], label=label)\n",
        "\n",
        "_ = ax.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl6m0B6iSagp"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEwBspLySagp"
      },
      "source": [
        "# Question 3: Who bombed this location?\n",
        "\n",
        "<img style=\"float: right;\" src=\"https://raw.githubusercontent.com/epfl-ada/2019/c17af0d3c73f11cb083717b7408fedd86245dc4d/Tutorials/04%20-%20Scaling%20Up/img/Hanoi_POL1966.jpg\">\n",
        "\n",
        "This picture is the Hanoi POL facility (North Vietnam) burning after it was attacked by the U.S. Air Force on 29 June 1966 in the context of the Rolling Thunder operation. \n",
        "\n",
        "We are interested in discovering what was the most common take-off location during that day. Transform the operations dataframe to contain operations for the date in question, then cache and execute this caching."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2021-10-24T19:26:49.002Z"
        },
        "id": "2_OxFy2yk1KX"
      },
      "source": [
        "# your code goes here\n",
        "cond = (F.col('MissionDate') == '1966-06-29') & (F.col('TargetCountry') == 'NORTH VIETNAM')\n",
        "df_operations_jun_29 = df_operations.filter(cond)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qurjwd0zSagr"
      },
      "source": [
        "Which coutries scheduled operations that day?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2021-10-24T19:26:49.003Z"
        },
        "id": "uoiVSE7NSags"
      },
      "source": [
        "# your code goes here\n",
        "df_operations_jun_29.select(F.col('ContryFlyingMission')).distinct().show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0Q2bS4KSagt"
      },
      "source": [
        "How many operations where done per country on this date?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2021-10-24T19:26:49.004Z"
        },
        "id": "cLec-CkmSagu"
      },
      "source": [
        "# your code goes here\n",
        "df_operations_jun_29.groupby('ContryFlyingMission').count().show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rHUr9mek1KX"
      },
      "source": [
        "Time the previous query using the `collect` action. Then cache the dataframe containing operations from the date in question, invoke an action and time the previous query on this cached dataframe. How much faster is the query on your cached dataframe?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2021-10-24T19:26:49.005Z"
        },
        "id": "s4-REURtk1KY"
      },
      "source": [
        "# your code goes here\n",
        "%timeit df_operations_jun_29.groupby('ContryFlyingMission').count().collect()\n",
        "df_operations_jun_29.cache()\n",
        "df_operations_jun_29.count()\n",
        "%timeit df_operations_jun_29.groupby('ContryFlyingMission').count().collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4Hzke2kSag1"
      },
      "source": [
        "Write the cached dataframe to disk and read it back (you needn't assign it to a variable when reading it in)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2021-10-24T19:26:49.006Z"
        },
        "id": "4_2p0Ha9Sag1"
      },
      "source": [
        "# your code goes here\n",
        "df_operations_jun_29.write.json('operations_jun_29.json')\n",
        "spark.read.json('operations_jun_29.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEW08u3_Sag6"
      },
      "source": [
        "Show the number of operations per takeoff location, in descending order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2021-10-24T19:26:49.007Z"
        },
        "id": "UYO-CuHISag8"
      },
      "source": [
        "# your code goes here\n",
        "df_operations_jun_29.groupby('TakeoffLocation').count().sort(F.desc('count')).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cE8asfBPSahD"
      },
      "source": [
        "<img style=\"float: right;\" src=\"https://raw.githubusercontent.com/epfl-ada/2019/c17af0d3c73f11cb083717b7408fedd86245dc4d/Tutorials/04%20-%20Scaling%20Up/img/USS_Constellation.jpg\">\n",
        "\n",
        "\n",
        "That day the most common take-off location was the ship USS Constellation (CV-64). We cannot univocally identify one take off location, but we can reduce the possible candidates. Next steps: explore TimeOnTarget feature.\n",
        "\n",
        "_USS Constellation (CV-64), a Kitty Hawk-class supercarrier, was the third ship of the United States Navy to be named in honor of the \"new constellation of stars\" on the flag of the United States. One of the fastest ships in the Navy, as proven by her victory during a battlegroup race held in 1985, she was nicknamed \"Connie\" by her crew and officially as \"America's Flagship\"._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mv1OKc3k1KY"
      },
      "source": [
        "Finally, compute the mean, median first and third quantile of the `TimeOnTarget` value per takeoff location (you could make use of `F.percentile_approx` from pyspark 3.1.1 and up).  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2021-10-24T19:26:49.009Z"
        },
        "id": "ErERDJs-k1KY"
      },
      "source": [
        "# your code goes here\n",
        "df_operations_jun_29.groupby('TakeoffLocation') \\\n",
        "    .agg(F.mean('TimeOnTarget').alias('avg_tot'), \n",
        "         F.percentile_approx('TimeOnTarget', [0.25, 0.5, 0.75], 1000000).alias('quantiles')) \\\n",
        "    .sort(F.asc('avg_tot')) \\\n",
        "    .toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKKM6VS8SahD"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaiVcdF8SahD"
      },
      "source": [
        "# Question 4: What is the most used aircraft type during the Vietnam war (number of operations)?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnV48FZlSahG"
      },
      "source": [
        "Let's check the content of `Aircraft_Glossary`. Have a look at it using `show`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2021-10-24T19:26:49.010Z"
        },
        "id": "yovhBV5sSahI"
      },
      "source": [
        "# your code goes here\n",
        "df_aircraft.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yO0ExxRoSahJ"
      },
      "source": [
        "How many unique values do we have in the `AirCraftType` column? Display them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2021-10-24T19:26:49.011Z"
        },
        "id": "Z4DnpCTmSahJ",
        "scrolled": true
      },
      "source": [
        "# your code goes here\n",
        "df_aircraft.select('AirCraftType').distinct().toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z97Y9DuUSahK"
      },
      "source": [
        "Both our dataframes has a column called `AirCraft` (a shared key between the two). Join the dataframes together and name the result `df_operations_aircrafts`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2021-10-24T19:26:49.012Z"
        },
        "id": "s3j3Fu8kSahL",
        "scrolled": true
      },
      "source": [
        "# your code goes here\n",
        "df_operations_aircrafts = df_operations.join(df_aircraft, on='AirCraft', how='inner')\n",
        "df_operations_aircrafts.limit(10).toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhQbN8AOSahN"
      },
      "source": [
        "Determine the most used aircraft types for the entirety of the Vietnam War."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2021-10-24T19:26:49.013Z"
        },
        "id": "S0QytpygSahU"
      },
      "source": [
        "# your code goes here\n",
        "df_operations_aircrafts.groupby('AirCraftType').count().sort(F.desc('count')).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffrW-oV2SahW"
      },
      "source": [
        "Note: This dataset would require further cleaning and normalization. See `Fighter Jet Bomber`, `Jet Fighter Bomber`, `Fighter bomber jet`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2U5HkWYak1Ka"
      },
      "source": [
        "Determine which aircraft types were fastest (on average) to complete their operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2021-10-24T19:26:49.014Z"
        },
        "id": "vH8XhyvPk1Ka"
      },
      "source": [
        "# your code goes here\n",
        "df_operations_aircrafts.groupby('AirCraftType') \\\n",
        "    .agg(F.avg('TimeOnTarget').alias('avg_tot')) \\\n",
        "    .sort(F.asc('avg_tot')) \\\n",
        "    .toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFdOhgg8k1Ka"
      },
      "source": [
        "What join strategy was chosen by spark for this last query? (Inspect the Spark UI SQL query DAG). \n",
        "\n",
        "Finally, use the `hint` function to request spark to do a Shuffle Merge Sort Join (SMJ) for the previous query (see [here](https://towardsdatascience.com/about-joins-in-spark-3-0-1e0ea083ea86) for more information). Did the query become faster or slower?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2021-10-24T19:26:49.015Z"
        },
        "id": "laaFTcFsk1Ka"
      },
      "source": [
        "# your code goes here\n",
        "df_operations \\\n",
        "    .join(df_aircraft.hint('shuffle_merge'), on='AirCraft', how='inner') \\\n",
        "    .groupby('AirCraftType') \\\n",
        "    .agg(F.avg('TimeOnTarget').alias('avg_tot')) \\\n",
        "    .sort(F.asc('avg_tot')) \\\n",
        "    .toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdLsVtcSE6F9"
      },
      "source": [
        "# Additional material\n",
        "\n",
        "* An animated timeline of bombing operations is available [here](https://cdn.filepicker.io/api/file/el7zpgfhTOqapUblTvUh) and discussions about the dataset [here](https://data.world/datamil/vietnam-war-thor-data/discuss/vietnam-war-thor-data/gftdgyjz)."
      ]
    }
  ]
}